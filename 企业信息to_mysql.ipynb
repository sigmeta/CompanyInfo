{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests  \n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_path='2008a/hebei'\n",
    "all_path=os.listdir(start_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356874"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbHandle():\n",
    "    conn = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        passwd='9432',\n",
    "        charset='utf8',\n",
    "        use_unicode=False\n",
    "    )\n",
    "    return conn\n",
    "dbObject = dbHandle()\n",
    "cursor = dbObject.cursor()\n",
    "sql = 'insert into company.hebei(url,company,province,website,industry_b,headquarters,welfare,工商注册号,组织机构代码,统一信用代码,经营状态,行业,企业类型,法定代表人,营业期限,注册资本,核准日期,登记机关,企业地址,经营范围,企业联系电话,电子邮箱,主要人员,股东信息,热招职位) values (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)'\n",
    "\n",
    "n=0\n",
    "while n<len(all_path):\n",
    "    print(n)\n",
    "    with open(start_path+'/'+all_path[n],'r',encoding='utf-8') as f:\n",
    "        soup=BeautifulSoup(f.read(),'lxml')\n",
    "        if not soup.find('span',class_=\"title line-clamp1\"):\n",
    "            print(all_path[n],'空')\n",
    "            n+=1\n",
    "            continue\n",
    "        item={}\n",
    "        item[\"url\"]=''\n",
    "        item[\"company\"]=''\n",
    "        item[\"website\"]=''\n",
    "        #item[\"province\"]=''\n",
    "        item[\"headquarters\"]=''\n",
    "        item[\"industry_b\"]=''\n",
    "        item[\"welfare\"]=''\n",
    "        item['工商注册号']=''\n",
    "        item['组织机构代码']=''\n",
    "        item['统一信用代码']=''\n",
    "        item['经营状态']=''\n",
    "        item['行业']=''\n",
    "        item['企业类型']=''\n",
    "        item['法定代表人']=''\n",
    "        item['营业期限']=''\n",
    "        item['注册资本']=''\n",
    "        item['核准日期']=''\n",
    "        item['登记机关']=''\n",
    "        item['企业地址']=''\n",
    "        item['经营范围']=''\n",
    "        item['企业联系电话']=''\n",
    "        item['电子邮箱']=''\n",
    "        item['主要人员']=''\n",
    "        item['股东信息']=''\n",
    "        item['热招职位']=''\n",
    "        \n",
    "        \n",
    "        item[\"province\"]='hebei'\n",
    "        \n",
    "        if soup.find('a',target=\"_blank\").text!='+关注':\n",
    "            item[\"website\"]=soup.find('a',target=\"_blank\").text\n",
    "        #item[\"url\"]=response.url\n",
    "        item[\"company\"]=soup.find('span',class_=\"title line-clamp1\").text.strip()\n",
    "        \n",
    "        t=soup.find('div',class_=\"base\").text\n",
    "        if re.search('总部地点',t):\n",
    "            item[\"headquarters\"]=t[re.search('总部地点',t).end()+1:re.search('总部地点\\S*',t).end()]\n",
    "        if re.search('所属行业',t):\n",
    "            item[\"industry_b\"]=t[re.search('所属行业',t).end()+1:re.search('所属行业\\S*',t).end()]\n",
    "        if re.search('公司福利',t):\n",
    "            item[\"welfare\"]=t[re.search('公司福利',t).end()+1:re.search('公司福利\\S*',t).end()]\n",
    "        if soup.find('div',class_=\"gszc\"):\n",
    "            for t in soup.find('div',class_=\"gszc\").div.find_all('td',class_=\"tr-title\"):\n",
    "                item[t.text[:-1]]=t.next_sibling.next_sibling.text\n",
    "        t=soup.find('div',class_=\"qynb\").find_all('td')\n",
    "        for tdn in range(len(t)):\n",
    "            if t[tdn].text=='电子邮箱：':\n",
    "                item[\"电子邮箱\"]=t[tdn+1].text\n",
    "            elif t[tdn].text=='企业联系电话：':\n",
    "                item[\"企业联系电话\"]=t[tdn+1].text\n",
    "\n",
    "        for t in soup.find('div',class_=\"gszc\").find_all('h2')[1:]:\n",
    "            if t.text=='主要人员':\n",
    "                st=''\n",
    "                for td in t.next_sibling.next_sibling.find_all('td',class_=\"tr-title\"):\n",
    "                    st+=td.text+td.next_sibling.next_sibling.text+';'\n",
    "                item[\"主要人员\"]=st[:-1]\n",
    "                    \n",
    "            elif t.text=='股东信息':\n",
    "                st=''\n",
    "                if len(t.next_sibling.next_sibling.find_all('tr'))>1:\n",
    "                    for tr in t.next_sibling.next_sibling.find_all('tr')[1:]:\n",
    "                        for td in tr.find_all('td'):\n",
    "                            st+=td.text+','\n",
    "                        st=st[:-1]+';'\n",
    "                item[\"股东信息\"]=st[:-1]\n",
    "        st=''\n",
    "        for a in soup.find('div',class_=\"position\").find_all('a'):\n",
    "            if not a.span:\n",
    "                break\n",
    "            st+=a.span.text+','\n",
    "            for p in a.find_all('p'):\n",
    "                st+=p.text+','\n",
    "            st=st[:-1]+';'\n",
    "        item[\"热招职位\"]=st[:-1]\n",
    "        \n",
    "        \n",
    "\n",
    "        try:\n",
    "            cursor.execute(sql,(item['url'],item['company'],item['province'],item['website'],item['industry_b'],item['headquarters'],item['welfare'],item['工商注册号'],item['组织机构代码'],item['统一信用代码'],item['经营状态'],item['行业'],item['企业类型'],item['法定代表人'],item['营业期限'],item['注册资本'],item['核准日期'],item['登记机关'],item['企业地址'],item['经营范围'],item['企业联系电话'],item['电子邮箱'],item['主要人员'],item['股东信息'],item['热招职位']))\n",
    "            dbObject.commit()\n",
    "            #print('写入成功')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('###########写入失败！############')\n",
    "            dbObject.rollback()\n",
    "            \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        djson={}\n",
    "        if soup.find('table',class_='gdxx'):\n",
    "            djson['title']=[t.text for t in soup.find('table',class_='gdxx').find('tr').find_all('th')]\n",
    "            djson['list']=[]\n",
    "            for tlist in soup.find('table',class_='gdxx').find_all('tr')[1:]:\n",
    "                djson['list'].append([t.text for t in tlist.find_all('td')])\n",
    "        df.loc[n,'gdxx']=json.dumps(djson)\n",
    "        '''\n",
    "    \n",
    "    n+=1\n",
    "print('完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a78e151884335688f0fe09a87e7ac231.txt'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path[137785]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
